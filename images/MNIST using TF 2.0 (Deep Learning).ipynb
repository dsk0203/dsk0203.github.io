{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dylan\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 14:40:48.597503  6092 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from download\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# create a validation set, INT format\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# number of test samples was stored in num_examples from before (info data)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#need to scale down, we know all contained 0-255 values in grey-scale matrix, this would create values 0 to 1\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "#apply this scale to the main dataset\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# apply to test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# shuffle the data and extract the buffer size for splitting\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "\n",
    "# extract train and validation data\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "#skip the initial validation numbers\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#mini batch gradient descent\n",
    "# batch size = 1, Stochastic Gradient Descent\n",
    "# batch size = number of samples = single batch Gradient Descent\n",
    "# 1 < Batch Size < samples = mini-batch gradient descent\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#combine to batches\n",
    "\n",
    "train_data =train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# don't batch validation set or test set, won't be back propogating\n",
    "\n",
    "# so set the validation data to a batch = to the acutal size\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# load inputs and validation targets\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "#Hyperparameters\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "\n",
    "# Sequential used to start layering the model (STACK LAYERS)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # flatten the pics\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    # dot product math to find the bias and weights + activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    # second layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    # third layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='selu'),\n",
    "    # output layer want probabilities so use softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the optimizer and the loss function\n",
    "\n",
    "# ready the model for trainning\n",
    "# loss functions for classification\n",
    "# binary_crossentropy - binary data\n",
    "# categorical_crossentropy - one-hot encoded targets\n",
    "# sparse_categorical_crossentropy - will apply one-hot encoding\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2377 - accuracy: 0.9294 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "540/540 - 5s - loss: 0.0936 - accuracy: 0.9712 - val_loss: 0.0805 - val_accuracy: 0.9772\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.0624 - accuracy: 0.9799 - val_loss: 0.0921 - val_accuracy: 0.9738\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.0506 - accuracy: 0.9847 - val_loss: 0.0533 - val_accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.0418 - accuracy: 0.9862 - val_loss: 0.0433 - val_accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26f85f2fe88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainning\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# validation include as an agrument equal to validation inputs\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "# HOW IT WORKS\n",
    "# 1.) At the beginning of each epoch, the traininng loss wil be set to 0\n",
    "# 2.) The algorithm will iterate over a preset number of batches, all from train_data\n",
    "# 3.) The weights and biases will be updated as many times as there are batches\n",
    "# 4.) We will get a value for the loss function, indicating how the trainning is going\n",
    "# 5.) We will also see a trainning accuracy\n",
    "# 6.) At the end of the epoch, the algorithm will forward propagate the whole validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 98.58 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 837ms/step - loss: 0.0882 - accuracy: 0.976 - 1s 843ms/step - loss: 0.0882 - accuracy: 0.9762"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "#forward propgate through the net\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08818195760250092   0.9762\n"
     ]
    }
   ],
   "source": [
    "print(test_loss, \" \" , test_accuracy)\n",
    "\n",
    "\n",
    "#no longer cna opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after running on test data can't optimize, as it's seen the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 97.62 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
